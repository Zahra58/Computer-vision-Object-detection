{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1x5GJab8w1V"
   },
   "source": [
    "# **Computer vision Object detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00dce9c7"
   },
   "source": [
    "# Object Detection and Tracking using YOLOv8 and IoU Tracker\n",
    "\n",
    "This project demonstrates an end-to-end pipeline for object detection and tracking in a video using the YOLOv8 pre-trained CNN model and a simple Intersection over Union (IoU) based tracker.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "*   **Object Detection:** Utilizes YOLOv8 (trained on the MS COCO dataset) to identify multiple everyday objects in video frames.\n",
    "*   **Object Tracking:** Implements a lightweight IoU tracker to maintain consistent identities for detected objects across consecutive frames.\n",
    "*   **Data Collection & Preparation:** Includes steps for video loading, frame sampling, and ground truth annotation preparation.\n",
    "*   **Evaluation:** Provides a comprehensive evaluation of the object detection model's performance using Precision, Recall, F1-score, Average Precision (AP), and mean Average Precision (mAP) against manual ground truth annotations.\n",
    "*   **Visualization:** Generates visualizations including sample video frames with detections/tracking, class frequency plots, and Precision-Recall curves.\n",
    "\n",
    "**Project Steps:**\n",
    "\n",
    "1.  Data Collection: Load and verify input video.\n",
    "2.  Object Detection & Tracking: Apply YOLOv8 and the IoU tracker to the video.\n",
    "3.  Ground Truth Preparation: Extract frames and prepare for manual annotation.\n",
    "4.  Model Evaluation: Compare YOLOv8 detections against ground truth using standard metrics (Precision, Recall, F1, AP, mAP).\n",
    "5.  Visualization: Generate plots to analyze model performance and data characteristics.\n",
    "\n",
    "This project provides a foundation for understanding and implementing basic object detection and tracking workflows using popular deep learning techniques and evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4945,
     "status": "ok",
     "timestamp": 1762293570769,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "tqrdEnqY8pUI",
    "outputId": "a527d40b-335d-4a26-c639-b72ca23fc5f1"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics opencv-python numpy scipy\n",
    "\n",
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mBva5n6QkU8"
   },
   "source": [
    "Task 1 = Data collection\n",
    "I collected a video according.\n",
    "\n",
    "Next block of codes will show the original video here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 15873,
     "status": "ok",
     "timestamp": 1762293595576,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "K9Mx3eYIQd6j",
    "outputId": "a1a03596-76e1-412b-ab85-a735c112f116"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "video_path = '/content/object-detection-raw.mp4'\n",
    "\n",
    "try:\n",
    "    # Attempt to display the video directly\n",
    "    display(Video(video_path, embed=True, width=720))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display video directly: {e}\")\n",
    "    print(\"Attempting to fix with ffmpeg...\")\n",
    "    # If direct display fails, try fixing with ffmpeg\n",
    "    output_path_fixed = '/content/task1_fixed.mp4'\n",
    "    !ffmpeg -i {video_path} -vcodec libx264 -acodec aac -y {output_path_fixed}\n",
    "    try:\n",
    "        display(Video(output_path_fixed, embed=True, width=720))\n",
    "        print(f\"Video displayed after fixing with ffmpeg.\")\n",
    "    except Exception as e_fixed:\n",
    "        print(f\"Could not display video even after fixing: {e_fixed}\")\n",
    "        print(f\"You can try downloading the video from {video_path} or {output_path_fixed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0a5jvajY0nd"
   },
   "source": [
    "# **Object tracking and recognition**\n",
    "Now let us implement a solution to detect, recognize, and track moving objects in the video using a pre-trained CNN model. First I will load the video using OpenCV and initializing a pre-trained object detection model (YOLO ) to process each frame.\n",
    "\n",
    "Task 2: Object detection + tracking pipeline for my video at /content/object-detection-raw.mp4.\n",
    "It uses YOLOv8 (pretrained COCO) for detections and a lightweight IoU tracker (greedy matching) to keep stable IDs across frames. The result is saved as /content/task2.mp4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2KTRkMKaGwX"
   },
   "source": [
    "In the next cell I will install :\n",
    "\n",
    "!pip install ... â†’ Installs these libraries inside your Colab environment.\n",
    "\n",
    "ultralytics â†’ contains the YOLOv8 model for object detection (by Ultralytics).\n",
    "\n",
    "opencv-python (cv2) â†’ for reading/writing videos, drawing bounding boxes, and image processing.\n",
    "\n",
    "numpy â†’ provides fast numerical operations (arrays, matrices, coordinates).\n",
    "\n",
    "scipy â†’ includes scientific utilities; here itâ€™s used for computing distances (e.g., for object tracking).\n",
    "\n",
    "matplotlib â†’ for visualising images, graphs, and bounding boxes.\n",
    "\n",
    "seaborn â†’ a higher-level plotting library built on top of Matplotlib, great for heatmaps or evaluation plots.\n",
    "\n",
    "pandas â†’ for handling tabular data, such as detection results and evaluation CSVs.\n",
    "\n",
    "scikit-learn â†’ for evaluation metrics like confusion matrices, precisionâ€“recall curves, and average precision.\n",
    "\n",
    "This is like installing our entire â€œdata science toolkitâ€ + YOLO engine.\n",
    "# **why I chose YOLO**\n",
    "Model Choice: YOLOv8\n",
    "\n",
    "YOLOv8 was chosen as the pre-trained CNN model for object detection and tracking. It is a convolutional neural network trained on the MS COCO dataset, which includes 91 everyday object categories such as person, cup, chair, and laptop. This makes it directly compatible with the task requirement to detect multiple MS COCO classes. YOLOv8 provides an excellent balance between speed and accuracy, enabling real-time detection and easy integration with IoU-based tracking methods. Compared to alternatives like Faster R-CNN or SSD, YOLOv8 offers faster inference and simpler implementation for video-based applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4638,
     "status": "ok",
     "timestamp": 1762293698683,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "Q2HyP-nUTy5V",
    "outputId": "9fef2e6b-2792-478d-d8ec-399489132945"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install ultralytics opencv-python numpy scipy matplotlib seaborn pandas scikit-learn\n",
    "\n",
    "# Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('/content/evaluation', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWjNHZjjamXH"
   },
   "source": [
    "The next block verifies and summarises our input video before any processing:\n",
    "\n",
    "Confirms that the video loads correctly\n",
    "\n",
    "Prints basic metadata (size, FPS, duration)\n",
    "\n",
    "Ensures the video is suitable for Tasks 2 and 3\n",
    "\n",
    "This step verified the integrity and properties of the collected video. Using OpenCVâ€™s VideoCapture, the script extracted frame rate, resolution, and duration to ensure the data met the assessmentâ€™s recording criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1762293714965,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "PMktSLj2anSO",
    "outputId": "5d99252d-a662-4c41-e5de-79987cf2c4b4"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 1: DATA COLLECTION - VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 1: DATA COLLECTION - VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "video_path = '/content/object-detection-raw.mp4'  # Corrected video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {video_path}\")\n",
    "else:\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"Video properties:\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Total frames: {total_frames}\")\n",
    "    print(f\"  Duration: {total_frames/fps:.2f} seconds\")\n",
    "\n",
    "    cap.release() # Release the capture object after getting properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK5SR8b5a_7F"
   },
   "source": [
    "Next cell lets us:\n",
    "\n",
    "Visually verify my video quality (lighting, stability, visible objects, etc.).\n",
    "\n",
    "Check that myself and at least five MS COCO objects appear as required.\n",
    "\n",
    "Confirm the video isnâ€™t blurry or too dark before using YOLO for detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 2913,
     "status": "ok",
     "timestamp": 1762293724244,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "xrykMBaJbIzp",
    "outputId": "6a6213f6-85dd-4e6a-a42b-8ef2195c4098"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Path to your uploaded video\n",
    "video_path = '/content/object-detection-raw.mp4' # Make sure this path is correct\n",
    "\n",
    "# How many random frames to show\n",
    "num_samples = 4\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {video_path}\")\n",
    "else:\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Random frame indices (spread across the video)\n",
    "    sample_frames = sorted(random.sample(range(frame_count), num_samples))\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    for i, fidx in enumerate(sample_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, fidx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(5, 5,i+1)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Frame {fidx}\")\n",
    "\n",
    "    plt.suptitle(\"Sample Frames from Uploaded Video\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw59gNErbdIE"
   },
   "source": [
    "Explanation for next cell to see what is happening:\n",
    "\n",
    "Loads the YOLOv8-M (medium) model pre-trained on the MS COCO dataset.\n",
    "\n",
    "This is our pre-trained CNN required by the assessment.\n",
    "\n",
    "It can detect 80 COCO classes such as person, cup, chair, bottle, laptop, etc.\n",
    "\n",
    "Using this model meets two major criteria:\n",
    "\n",
    "â€œUse a pre-trained CNN model for object recognition.â€\n",
    "\n",
    "â€œDetect MS COCO categories in our video.â€\n",
    "\n",
    "Next cell explanation:\n",
    "\n",
    "Creates a lightweight tracker that matches detected objects frame-to-frame based on how much their bounding boxes overlap (Intersection over Union, IoU).\n",
    "\n",
    "iou_threshold: how much overlap two boxes must have to be considered the same object.\n",
    "\n",
    "max_age: how many frames a track can go unmatched before being deleted. Computes the IoU score between two bounding boxes. The formula is:\n",
    "\n",
    "ð¼ ð‘œ ð‘ˆ\n",
    "area of intersection area of union IoU= area of union area of intersectionâ€‹\n",
    "\n",
    "This tells how much two boxes overlap (1 = perfect overlap, 0 = none).\n",
    "\n",
    "Each frame, this function:\n",
    "\n",
    "Ages existing tracks â€” if a track hasnâ€™t been matched for more than max_age frames, itâ€™s deleted.\n",
    "\n",
    "Compares each new detection against current tracks using IoU.\n",
    "\n",
    "Updates existing tracks if IoU > threshold.\n",
    "\n",
    "Creates new track IDs for any unmatched detections.\n",
    "\n",
    "This ensures every object gets a consistent ID while it moves through the video.\n",
    "\n",
    "Loads your video (replace with /content/22534699.mp4).\n",
    "\n",
    "Sets the video codec (mp4v) and prepares to save results to task2.mp4.\n",
    "\n",
    "Runs the YOLOv8 model on each frame.\n",
    "\n",
    "conf=0.5 â†’ minimum confidence threshold for detections.\n",
    "\n",
    "iou=0.5 â†’ non-maximum suppression threshold to remove duplicate boxes.\n",
    "\n",
    "YOLO returns bounding boxes, class IDs, and confidence scores for every detected object.\n",
    "\n",
    "Outcome\n",
    "A fully processed video task2.mp4 containing:\n",
    "\n",
    "YOLOv8 object detections (COCO categories).\n",
    "\n",
    "Tracked objects with consistent IDs across frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1762293732164,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "Xk5Sl-BFbftu",
    "outputId": "febbf1a8-9ec3-4d1a-d191-77483b34b824"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 2: OBJECT DETECTION AND TRACKING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TASK 2: Object Detection and Tracking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load video\n",
    "video_path = '/content/object-detection-raw.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video: {video_path}\")\n",
    "print(f\"Resolution: {width}x{height}, FPS: {fps}, Total frames: {total_frames}\")\n",
    "\n",
    "# Initialize YOLO model (YOLOv8 pretrained on COCO)\n",
    "print(\"Loading YOLOv8 model...\")\n",
    "model = YOLO('yolov8m.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrkVQ1aOcNgX"
   },
   "source": [
    "The next cell defines a simple IoU (Intersection over Union) tracker class and then uses it in conjunction with the loaded YOLOv8 model to perform object detection and tracking on your video.\n",
    "\n",
    "Here's a breakdown of what the code does:\n",
    "\n",
    "SimpleIOUTracker Class: This class implements a basic tracking algorithm. It keeps track of objects detected in previous frames and tries to match them with new detections in the current frame based on their bounding box overlap (IoU). iou_threshold: This parameter determines how much overlap is needed for a new detection to be considered the same object as an existing track. max_age: If a track is not matched with a detection for this many frames, it is considered lost and removed. The update method is the core of the tracker, taking new detections and returning the updated list of tracked objects with unique IDs. Video Processing Loop: It opens your video (/content/object-detection-raw.mp4). It initializes a VideoWriter to save the output video (/content/task2.mp4). It reads the video frame by frame. For each frame, it runs the YOLOv8 model to get object detections. These detections are then passed to the tracker.update() method to get tracked objects with consistent IDs. It draws the bounding boxes, class names, and unique track IDs on the frame. The modified frame is written to the output video file. It includes print statements to show the progress (processed frames). Interpretation of the Output:\n",
    "\n",
    "The output we see in the cell shows the progress of the video processing.\n",
    "\n",
    "Video: /content/object-detection-raw.mp4: Confirms that the correct video file is being processed. Resolution: 464x848, FPS: 30, Total frames: : Shows the properties of the input video. Loading YOLOv8 model...: Indicates that the pre-trained YOLOv8 model is being loaded. Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt...: Shows that the model weights are being downloaded.\n",
    "\n",
    "Lines like 0: 640x352 1 oven, 696.9ms indicate that YOLOv8 is processing frames. 640x352 is the internal processing resolution, and 1 oven means it detected one object classified as \"oven\". The time in milliseconds is the inference time for that frame. Processed XXX/1764 frames: These lines show the progress of the video processing loop, indicating how many frames have been read, detected, tracked, and written to the output video. Finished processing loop. Total frames processed: 1764: Confirms that the code has gone through all frames of the video. Task 2 complete! Video saved to /content/task2.mp4: Indicates that the process finished successfully and the output video with detections and tracking is saved. The output confirms that the object detection and tracking pipeline ran successfully on our video, and the results are saved in the /content/task2.mp4 file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395449,
     "status": "ok",
     "timestamp": 1762294134409,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "jtD91DZfcjpR",
    "outputId": "7cec330f-5b74-46b0-b061-381a999797ab"
   },
   "outputs": [],
   "source": [
    "# IoU-based tracker\n",
    "class SimpleIOUTracker:\n",
    "    def __init__(self, iou_threshold=0.3, max_age=30):\n",
    "        self.tracks = {}  # {track_id: [boxes, confidences, class_ids]}\n",
    "        self.track_counter = 0\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.max_age = max_age\n",
    "        self.track_age = defaultdict(int)\n",
    "\n",
    "    def iou(self, box1, box2):\n",
    "        \"\"\"Calculate IoU between two boxes [x1, y1, x2, y2]\"\"\"\n",
    "        x1_min, y1_min, x1_max, y1_max = box1\n",
    "        inter_xmin = max(x1_min, box2[0])\n",
    "        inter_ymin = max(y1_min, box2[1])\n",
    "        inter_xmax = min(x1_max, box2[2])\n",
    "        inter_ymax = min(y1_max, box2[3])\n",
    "\n",
    "        if inter_xmax < inter_xmin or inter_ymax < inter_ymin:\n",
    "            return 0.0\n",
    "\n",
    "        inter_area = (inter_xmax - inter_xmin) * (inter_ymax - inter_ymin)\n",
    "        box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "    def update(self, detections):\n",
    "        \"\"\"Update tracker with new detections\n",
    "        detections: list of (box, conf, class_id)\n",
    "        \"\"\"\n",
    "        # Reset age for all tracks\n",
    "        for track_id in list(self.tracks.keys()):\n",
    "            self.track_age[track_id] += 1\n",
    "            if self.track_age[track_id] > self.max_age:\n",
    "                del self.tracks[track_id]\n",
    "                del self.track_age[track_id]\n",
    "\n",
    "        matched_tracks = set()\n",
    "        matched_detections = set()\n",
    "\n",
    "        # Match detections to existing tracks\n",
    "        for det_idx, (det_box, det_conf, det_class) in enumerate(detections):\n",
    "            best_iou = 0\n",
    "            best_track_id = None\n",
    "\n",
    "            for track_id, track_data in self.tracks.items():\n",
    "                track_box = track_data['box']\n",
    "                iou_score = self.iou(det_box, track_box)\n",
    "\n",
    "                if iou_score > best_iou and iou_score > self.iou_threshold:\n",
    "                    best_iou = iou_score\n",
    "                    best_track_id = track_id\n",
    "\n",
    "            if best_track_id is not None:\n",
    "                self.tracks[best_track_id] = {\n",
    "                    'box': det_box,\n",
    "                    'conf': det_conf,\n",
    "                    'class_id': det_class\n",
    "                }\n",
    "                self.track_age[best_track_id] = 0\n",
    "                matched_tracks.add(best_track_id)\n",
    "                matched_detections.add(det_idx)\n",
    "\n",
    "        # Create new tracks for unmatched detections\n",
    "        for det_idx, (det_box, det_conf, det_class) in enumerate(detections):\n",
    "            if det_idx not in matched_detections:\n",
    "                self.track_counter += 1\n",
    "                self.tracks[self.track_counter] = {\n",
    "                    'box': det_box,\n",
    "                    'conf': det_conf,\n",
    "                    'class_id': det_class\n",
    "                }\n",
    "                self.track_age[self.track_counter] = 0\n",
    "\n",
    "        return self.tracks\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = SimpleIOUTracker(iou_threshold=0.3, max_age=30)\n",
    "\n",
    "# Process video for object detection and tracking\n",
    "output_path_task2 = '/content/task2.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# Re-initialize video capture here\n",
    "video_path = '/content/object-detection-raw.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties again after re-initializing cap\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file {video_path} for processing.\")\n",
    "else:\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Create VideoWriter only if cap is opened successfully\n",
    "    out = cv2.VideoWriter(output_path_task2, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    print(\"Processing video for object detection and tracking...\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Warning: Could not read frame {frame_count}. Exiting loop.\") # Added print\n",
    "            break\n",
    "\n",
    "        # Run YOLO detection\n",
    "        results = model(frame, conf=0.5, iou=0.5)\n",
    "\n",
    "        detections = []\n",
    "        if results[0].boxes is not None:\n",
    "            for box in results[0].boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                conf = float(box.conf[0])\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = model.names[class_id]\n",
    "                detections.append(([x1, y1, x2, y2], conf, class_id))\n",
    "\n",
    "        # Update tracker\n",
    "        tracks = tracker.update(detections)\n",
    "\n",
    "        # Draw results on frame\n",
    "        frame_display = frame.copy()\n",
    "\n",
    "        for track_id, track_data in tracks.items():\n",
    "            x1, y1, x2, y2 = track_data['box']\n",
    "            conf = track_data['conf']\n",
    "            class_id = track_data['class_id']\n",
    "            class_name = model.names[class_id]\n",
    "\n",
    "            # Draw bounding box (changed color to red)\n",
    "            color = (0, 0, 255) # Red color in BGR\n",
    "            cv2.rectangle(frame_display, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "            # Draw label with track ID\n",
    "            label = f\"ID: {track_id} {class_name} {conf:.2f}\"\n",
    "            cv2.putText(frame_display, label, (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        out.write(frame_display)\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count % 10 == 0:\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames\") # Added print\n",
    "\n",
    "    print(f\"Finished processing loop. Total frames processed: {frame_count}\") # Added print\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Task 2 complete! Video saved to {output_path_task2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-89brdsfAFf"
   },
   "source": [
    "I tried to show the video in the next cell but I encounter an issue. google COLAB would'nt show the video then I had to change some parametters to fix the issue.\n",
    "\n",
    "Colabâ€™s built-in video preview sometimes canâ€™t render videos with:\n",
    "\n",
    "Unusual frame sizes (my video is 464Ã—848, which is tall).\n",
    "\n",
    "Certain codecs (mp4v or H.264 wrapped differently).\n",
    "\n",
    "Large file sizes (>50 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 27893,
     "status": "ok",
     "timestamp": 1762294200094,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "Qra6xhUAfB6r",
    "outputId": "93cdf685-ba85-439d-debe-b9c31241c8e8"
   },
   "outputs": [],
   "source": [
    "!ffmpeg -i /content/task2.mp4 -vcodec libx264 -acodec aac /content/task2_fixed.mp4 -y\n",
    "from IPython.display import Video\n",
    "Video(\"/content/task2_fixed.mp4\", embed=True, width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpFab_czhybr"
   },
   "source": [
    "## **In the next cell we count unique COCO classes across the video**\n",
    "helps us confirm if we hit â‰¥5 categories for Task 1:\n",
    "\n",
    "### **The YOLOv8 model detected 24 unique MS COCO categories in the video, including person, chair, cup, laptop, clock, bottle, tv, and more, confirming the dataset meets Task 1 criteria.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130716,
     "status": "ok",
     "timestamp": 1762294337742,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "nNXBnaG7h2Bw",
    "outputId": "ed643519-8575-4450-eb6b-a8f39193a029"
   },
   "outputs": [],
   "source": [
    "import cv2, random\n",
    "from ultralytics import YOLO\n",
    "\n",
    "vid = \"/content/object-detection-raw.mp4\"\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "cap = cv2.VideoCapture(vid)\n",
    "n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "\n",
    "step = max(n//150, 1)  # sample ~150 frames across the video\n",
    "seen = set()\n",
    "i = 0\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    if i % step == 0:\n",
    "        r = model.predict(frame, conf=0.35, iou=0.5, imgsz=640, verbose=False)\n",
    "        for b in r[0].boxes:\n",
    "            seen.add(model.names[int(b.cls)])\n",
    "    i += 1\n",
    "cap.release()\n",
    "print(\"Unique classes seen:\", sorted(seen), \" (count:\", len(seen), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8khADlsBiv6O"
   },
   "source": [
    "Quick code to get class counts (for a table/figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "executionInfo": {
     "elapsed": 131316,
     "status": "ok",
     "timestamp": 1762294829665,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "pwTj7eBpixGJ",
    "outputId": "03ac9a4f-f28d-4b7f-911d-eb3299ed95be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# re-sample quickly (lighter than full run):\n",
    "from ultralytics import YOLO\n",
    "import cv2, collections\n",
    "\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "cap = cv2.VideoCapture(\"/content/object-detection-raw.mp4\")\n",
    "n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "step = max(n//150, 1)\n",
    "counts = collections.Counter()\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    if i % step == 0:\n",
    "        r = model.predict(frame, conf=0.35, iou=0.5, imgsz=640, verbose=False)\n",
    "        for b in r[0].boxes:\n",
    "            counts[model.names[int(b.cls)]] += 1\n",
    "    i += 1\n",
    "cap.release()\n",
    "\n",
    "df = pd.DataFrame(counts.most_common(), columns=[\"class\",\"count\"])\n",
    "display(df.head(15))  # top classes for the report\n",
    "df.to_csv(\"/content/class_counts_sampled.csv\", index=False)\n",
    "print(\"Saved to /content/class_counts_sampled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiiVM-edmHLZ"
   },
   "source": [
    "# Task 1 â€“ Video Dataset Summary\n",
    "\n",
    "The uploaded video (object-detection-raw.mp4) was analyzed using the YOLOv8 pre-trained CNN model trained on the MS COCO dataset. The model detected 24 unique object categories across sampled frames, confirming the presence of multiple MS COCO classes as required.\n",
    "The most frequently detected objects were dining table (134), laptop (93), cup (92), wine glass (84), knife (76), and spoon (69).\n",
    "This demonstrates that the video provides sufficient visual variety and contains several everyday objects, ensuring a robust dataset for object detection and tracking tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1762294852270,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "mT6vZL9omwqp",
    "outputId": "8312021c-ce5e-40a4-8021-da2e8b66f6c2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your saved results\n",
    "df = pd.read_csv(\"/content/class_counts_sampled.csv\")\n",
    "\n",
    "# Sort by count descending\n",
    "df = df.sort_values(by=\"count\", ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"count\", y=\"class\", data=df, palette=\"viridis\")\n",
    "\n",
    "plt.title(\"Detected Object Categories and Their Frequency\", fontsize=14)\n",
    "plt.xlabel(\"Count\", fontsize=12)\n",
    "plt.ylabel(\"Object Class\", fontsize=12)\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrHKVkNCnAKF"
   },
   "source": [
    "Figure X. Detected Object Categories and Frequency\n",
    "\n",
    "The horizontal bar chart above shows the frequency of objects detected in the recorded video using the YOLOv8 model pre-trained on the MS COCO dataset. The model identified 24 distinct object categories, with chair, person, and dining table being the most frequent. This confirms that the dataset satisfies the assignment requirement of containing at least five MS COCO categories and provides a suitable basis for object detection and tracking analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1762294863440,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "l5N2Bsh4nHax",
    "outputId": "2a39c7df-93d3-4703-fbb8-10eef616fcc5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/content/class_counts_sampled.csv\")\n",
    "\n",
    "# Sort and take top 10 classes\n",
    "df_sorted = df.sort_values(by=\"count\", ascending=False)\n",
    "top_df = df_sorted.head(10)\n",
    "others = pd.DataFrame([{\"class\": \"Others\", \"count\": df_sorted[\"count\"].iloc[10:].sum()}])\n",
    "df_pie = pd.concat([top_df, others])\n",
    "\n",
    "# Generate distinct colors using a high-contrast colormap\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(df_pie)))\n",
    "\n",
    "# Optional: explode top 3 categories slightly for emphasis\n",
    "explode = [0.05 if i < 3 else 0 for i in range(len(df_pie))]\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.pie(\n",
    "    df_pie[\"count\"],\n",
    "    labels=df_pie[\"class\"],\n",
    "    colors=colors,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140,\n",
    "    explode=explode,\n",
    "    shadow=False,\n",
    "    textprops={\"fontsize\": 10}\n",
    ")\n",
    "plt.title(\"Proportional Distribution of Top Object Categories\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-FM5YWwnVL3"
   },
   "source": [
    "The next cell, is designed to help us visually verify the object detection and tracking results from Task 2, which were saved to the /content/task2.mp4 video.\n",
    "\n",
    "Here's what the code does:\n",
    "\n",
    "Set up: We define the path to the output video from Task 2 (/content/task2.mp4) and specify that we want to sample and display 10 random frames from it. Video Loading and Check: I open the /content/task2.mp4 video using OpenCV. I include checks to ensure the file exists and can be opened successfully. Frame Sampling: If the video is valid, I get the total number of frames. We then randomly select 10 unique frame indices spread across the video's duration. Display Sampled Frames: I set up a matplotlib figure to display the images. We then loop through our selected random frame indices. For each index, I read the corresponding frame from the video. I convert the frame from BGR (OpenCV's default) to RGB format for correct display with matplotlib. Finally, I display the frame as a subplot within our figure, with the frame number as the title and removing the axes for a cleaner look. Final Touches: I add a main title to the entire figure and ensure the layout is tight so the subplots don't overlap. The video capture object is then released. Interpretation of the Output:\n",
    "\n",
    "The output we see after executing this cell consists of a matplotlib figure displaying 10 images. Each image is a frame taken from the /content/task2.mp4 video at a random index.\n",
    "\n",
    "We can observe the bounding boxes (drawn in red by the Task 2 code) around the detected objects. Each bounding box should have a label indicating the detected class (e.g., \"person\", \"chair\") and a unique track ID (e.g., \"ID: 5\"). By visually inspecting these frames, we can get a sense of how well our YOLOv8 model performed in detecting objects and how consistently the simple IoU tracker maintained IDs for the same object across different frames. The output also confirms that the /content/task2.mp4 file was successfully created and contains the visual results of our object detection and tracking pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 2280,
     "status": "ok",
     "timestamp": 1762294876449,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "NjAAmw-qnX1I",
    "outputId": "9d546060-8ded-40c9-9cc0-0bd34c742e8e"
   },
   "outputs": [],
   "source": [
    "#visualization of the model detection results\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Path to our output video with object detections\n",
    "video_path = '/content/task2.mp4'\n",
    "\n",
    "# How many random frames to show\n",
    "num_samples = 10 # Increased to 10\n",
    "\n",
    "# Check if the video file exists\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"Error: Video file not found at {video_path}. Please ensure the video was generated successfully in the previous steps.\")\n",
    "else:\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "    else:\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        if frame_count > 0:\n",
    "            # Random frame indices (spread across the video)\n",
    "            sample_frames = sorted(random.sample(range(frame_count), num_samples))\n",
    "\n",
    "            # Change figsize for a horizontal layout and increased frames\n",
    "            plt.figure(figsize=(20, 6)) # Adjusted figure size\n",
    "\n",
    "            for i, fidx in enumerate(sample_frames):\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, fidx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # Change subplot to a single row layout\n",
    "                plt.subplot(1, num_samples, i+1) # Changed to 1 row, num_samples columns\n",
    "                plt.imshow(frame_rgb)\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(f\"Frame {fidx}\\n(task2.mp4)\") # Added newline for clarity\n",
    "\n",
    "            plt.suptitle(\"Sample Frames from Object Detection Video (task2.mp4)\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Error: Video file {video_path} has no frames.\")\n",
    "\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWNQbfO-ny1F"
   },
   "source": [
    "The next cell is designed to help us download the frames that were just extracted from the ground truth video, so we can manually label them.\n",
    "\n",
    "Here's what this code does:\n",
    "\n",
    "Zip Frames: It uses the !zip -r command to create a compressed .zip file containing all the frames saved in the /content/ground_truth_frames directory. This makes it easy to download all the frames as a single file. The zip file is saved as /content/ground_truth_frames.zip. Provide Download Link: It uses Google Colab's files.download() function to trigger a download prompt in our browser for the created zip file. Interpretation of the Output:\n",
    "\n",
    "The output we see confirms that the frames were successfully zipped and made available for download:\n",
    "\n",
    "==============================================================================\n",
    "\n",
    "DOWNLOAD GROUND TRUTH FRAMES\n",
    "\n",
    "==============================================================================\n",
    "\n",
    "This heading indicates the purpose of the cell. adding: content/ground_truth_frames/ ...: These lines show the zip command working, listing the files it's adding to the archive. <IPython.core.display.Javascript object>: This output is from the files.download() function and represents the interactive element that triggers the browser download. We should see a file download prompt appear in our browser shortly after this output. Ground truth frames zipped and ready for download.: This confirms that the process completed successfully. This output indicates that our extracted ground truth frames are now compressed into a zip file, and the system has initiated a download for us. We can now download this zip file and use the frames for manual annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4383,
     "status": "ok",
     "timestamp": 1762294890124,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "a5wgsIP4n4mz",
    "outputId": "423c8f6b-6956-4acc-873f-86d46b972b4a"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GROUND TRUTH CREATION - MANUAL ANNOTATION\n",
    "# ============================================================================\n",
    "# I already created the ground truth mp4 manually. I will use this block of codes to extract CSV file for my ground truth for evaluation of my model\n",
    "import cv2, os\n",
    "import numpy as np\n",
    "\n",
    "video_path = \"/content/object-detection-raw.mp4\"\n",
    "output_dir = \"/content/ground_truth_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "\n",
    "# Extract one frame every second\n",
    "step = int(fps)\n",
    "frame_idx = 0\n",
    "saved = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if frame_idx % step == 0:\n",
    "        filename = os.path.join(output_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
    "        cv2.imwrite(filename, frame)\n",
    "        saved += 1\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"Extracted {saved} frames to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1762294895195,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "qO6snolv5xyQ",
    "outputId": "2fa8518f-764d-4b59-db2d-7e022926fbda"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD GROUND TRUTH FRAMES FOR MANUAL LABELLing\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: DOWNLOAD GROUND TRUTH FRAMES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Zip the directory containing the extracted frames\n",
    "!zip -r /content/ground_truth_frames.zip /content/ground_truth_frames\n",
    "\n",
    "# Provide a download link\n",
    "from google.colab import files\n",
    "files.download('/content/ground_truth_frames.zip')\n",
    "\n",
    "print(\"\\nGround truth frames zipped and ready for download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ2XCD3tqOfq"
   },
   "source": [
    "Ground Truth Preparation (Task 4)\n",
    "\n",
    "To evaluate the performance of object and face detection models, a manual ground truth dataset was created. A total of 59 frames were extracted from the recorded video (ground_truth.mp4) using OpenCV at one-frame-per-second intervals. These frames were manually annotated using makesense.ai , an open-source online annotation tool.\n",
    "\n",
    "Each frame was labeled with bounding boxes corresponding to MS COCO object categories (e.g., person, chair, cup, laptop, dining table) and faces visible in the scene. The annotations were exported in CSV format (ground_truth.csv) with columns:\n",
    "\n",
    "frame_idx, label, xmin, ymin, xmax, ymax\n",
    "\n",
    "Manual annotation was chosen instead of automatic labeling to ensure:\n",
    "\n",
    "Higher accuracy and consistency in bounding box placement\n",
    "\n",
    "Reliable comparison between YOLOv8 predictions (Task 2) and Haar-based face detections (Task 3)\n",
    "\n",
    "Faithful evaluation metrics (Precision, Recall, F1) for the final assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rwzw_hHe9rc4"
   },
   "source": [
    "Here are some tools commonly used for ground truth annotation:\n",
    "\n",
    "**Online Tools:**\n",
    "\n",
    "*   **makesense.ai:** (Used in this notebook) A free, open-source online tool that supports bounding boxes, polygons, and other annotation types. It's easy to use and doesn't require installation.\n",
    "*   **Labelbox:** A comprehensive data labeling platform that supports various data types (images, video, text, etc.) and annotation tasks. It offers collaboration features and quality control tools. (Paid)\n",
    "*   **CVAT (Computer Vision Annotation Tool):** An open-source web-based annotation tool for images and videos. It's powerful and supports various annotation types, including bounding boxes, polygons, and keypoints. It can be self-hosted.\n",
    "\n",
    "**Desktop Tools:**\n",
    "\n",
    "*   **LabelImg:** A free, open-source graphical image annotation tool for bounding boxes. It saves annotations as XML files in PASCAL VOC format or TXT files in YOLO format. (Requires installation)\n",
    "*   **VGG Image Annotator (VIA):** A simple and standalone manual annotation software for images, audio and video. It runs in a web browser but does not require a network connection.\n",
    "\n",
    "The choice of tool depends on your specific needs, including the type of data, the annotation task, the size of the dataset, team collaboration requirements, and budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1762294983741,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "Yi_AB_1sqRRY",
    "outputId": "65f1c114-3ab2-4e43-c84c-a3e3351ec3a7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Path to your manually annotated CSV\n",
    "csv_path = \"/content/ground_truth.csv.csv\"  # update path if needed\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(csv_path):\n",
    "    print(f\" File not found: {csv_path}\")\n",
    "else:\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    print(\" Ground Truth CSV loaded successfully!\")\n",
    "    print(\"\\nðŸ“„ First few rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Check required columns\n",
    "    # Updated expected columns to match the actual CSV structure\n",
    "    expected_cols = {\"image_name\", \"label_name\", \"bbox_x\", \"bbox_y\", \"bbox_width\", \"bbox_height\"}\n",
    "    if not expected_cols.issubset(df.columns):\n",
    "        print(f\" Missing columns! Expected: {expected_cols}\")\n",
    "    else:\n",
    "        print(\"\\nAll required columns found \")\n",
    "\n",
    "    # Summary by label\n",
    "    # Changed 'label' to 'label_name'\n",
    "    label_counts = df[\"label_name\"].value_counts().reset_index()\n",
    "    label_counts.columns = [\"label\", \"count\"] # Keep 'label' as column name for the plot\n",
    "\n",
    "    print(\"\\n Annotation count per class:\")\n",
    "    display(label_counts)\n",
    "\n",
    "    # Plot bar chart\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Changed 'label' to 'label' (from the renamed column) and 'count' to 'count'\n",
    "    sns.barplot(x=\"count\", y=\"label\", data=label_counts, palette=\"crest\", hue=\"label\", legend=False) # Added hue and legend=False\n",
    "    plt.title(\"Ground Truth Annotations by Class\", fontsize=14)\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Label\")\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN1gcthx9-JX"
   },
   "source": [
    "The nex cell is designed to detect objects on our 59 ground truth frames (the same images we used for manual annotation) using the YOLOv8 model. The purpose is to generate a set of detections that we can compare against our manual annotations for evaluation.\n",
    "\n",
    "Here's what this code does:\n",
    "\n",
    "Set Paths and Model: I define the directory where our ground truth frames are located (/content/ground_truth_frames), the path where the YOLO detections will be saved as a CSV (/content/eval_detections_yolo.csv), and the YOLO model weights to use (yolov8n.pt, the nano version). Load Model: I load the specified YOLO model (yolov8n.pt) once at the beginning. Process Frames and Detect: I initialize an empty list (records) to store the detection data. I get a sorted list of all JPG image files in the ground truth frames directory. I loop through each image file. For each image, I load it and run the YOLO model on it using specific confidence (conf=0.35) and IoU (iou=0.5) thresholds for non-maximum suppression. If the model finds any detections in the frame, I extract the bounding box coordinates (xyxy), class ID (cls), and confidence score (conf) for each detection. I convert the class ID to the corresponding class name using model.names. I append the detection details (filename, label, bounding box coordinates, confidence) as a row to the records list. Save Detections: After processing all frames, I convert the records list into a pandas DataFrame. I then save this DataFrame to a CSV file (/content/eval_detections_yolo.csv). Display Head: Finally, I print a message confirming the save location and the number of rows, and then display the first few rows of the created DataFrame. Interpretation of the Output:\n",
    "\n",
    "The output we see confirms that the YOLO detection process on the ground truth frames ran successfully:\n",
    "\n",
    "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt...: Shows that the YOLOv8 nano model weights are being downloaded. Saved detections to /content/eval_detections_yolo.csv (rows: 102): This is the key output. It confirms that the process finished and the detections were saved to the specified CSV file. It also tells us that a total of 102 object detections were made across all the processed ground truth frames. The displayed DataFrame head shows the structure of the saved data: image_name, label, x1, y1, x2, y2 (bounding box coordinates), and conf (confidence score). This output indicates that we have successfully generated a set of YOLOv8 detections for our ground truth frames, which is a necessary step for evaluating the performance of our object detection model against the manual annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1762295084426,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "O97_fW5H-BP4",
    "outputId": "e3e71101-a7cd-4c32-da6a-b00270798354"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Detect objects on your 59 frames (same images used for GT)\n",
    "# =========================================================\n",
    "import os, pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "FRAMES_DIR = \"/content/ground_truth_frames\"\n",
    "DET_CSV    = \"/content/eval_detections_yolo.csv\"\n",
    "MODEL_WTS  = \"yolov8n.pt\"\n",
    "\n",
    "# Load model once\n",
    "model = YOLO(MODEL_WTS)\n",
    "\n",
    "records = []\n",
    "frames = sorted([f for f in os.listdir(FRAMES_DIR) if f.lower().endswith(\".jpg\")])\n",
    "for fname in frames:\n",
    "    path = os.path.join(FRAMES_DIR, fname)\n",
    "    res = model(path, conf=0.35, iou=0.5, imgsz=640, verbose=False)[0]\n",
    "    if res.boxes is None:\n",
    "        continue\n",
    "    b = res.boxes\n",
    "    for bb, cc, cf in zip(b.xyxy.cpu().numpy(), b.cls.cpu().numpy().astype(int), b.conf.cpu().numpy()):\n",
    "        x1,y1,x2,y2 = [float(v) for v in bb]\n",
    "        label = model.names[int(cc)]\n",
    "        records.append([fname, label, x1, y1, x2, y2, float(cf)])\n",
    "\n",
    "det_df = pd.DataFrame(records, columns=[\"image_name\",\"label\",\"x1\",\"y1\",\"x2\",\"y2\",\"conf\"])\n",
    "det_df.to_csv(DET_CSV, index=False)\n",
    "print(f\" Saved detections to {DET_CSV} (rows: {len(det_df)})\")\n",
    "det_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLuH47Rj-V6v"
   },
   "source": [
    "The next cell is to normalize our manual ground truth CSV data to a standard format that is consistent with the format of the YOLO detections. This is a crucial step before we can perform a proper evaluation by comparing our model's predictions to the manual annotations.\n",
    "\n",
    "Here's what this code does:\n",
    "\n",
    "Define Paths: I set the paths for the raw ground truth CSV (/content/ground_truth.csv.csv) and the desired path for the standardized output CSV (/content/eval_ground_truth_std.csv). Load Raw GT Data: I load the raw manual annotation data from the specified CSV file into a pandas DataFrame (gt_raw). Normalize Bounding Box Format: Our raw annotations likely use a format like (x, y, width, height) for bounding boxes. This code converts that to the (x1, y1, x2, y2) format, which is commonly used and matches the format of the YOLO detections we generated. It creates a new DataFrame (gt_std) with the image name, label, and the new bounding box coordinates. Harmonize Labels (Optional but included): The code includes a step to clean up label names. In our case, it converts all labels to lowercase and fixes potential typos or variations (like changing \"microvalve\" to \"microwave\" and \"tvmonitor\" to \"tv\") to ensure consistent matching with YOLO's class names during evaluation. Save Standardized GT: The standardized ground truth data is saved to a new CSV file (/content/eval_ground_truth_std.csv). Print Summary and Head: I print a message confirming where the normalized data was saved and the number of rows in the new DataFrame. I also display the first few rows of the gt_std DataFrame. Interpretation of the Output:\n",
    "\n",
    "The output we see confirms that the ground truth data was successfully normalized and saved:\n",
    "\n",
    "Saved normalized GT to /content/eval_ground_truth_std.csv (rows: 207): This is the key output. It tells us that the normalization process finished and the standardized ground truth data was saved to the specified CSV file. The number of rows (207) indicates the total number of manually annotated objects across all the ground truth frames. The displayed DataFrame head shows the structure of the standardized data: image_name, label, x1, y1, x2, and y2. We can see that the bounding box coordinates are now in the desired (x1, y1, x2, y2) format, and the labels have been converted to lowercase. This output indicates that our manual ground truth data is now correctly formatted and ready to be used for evaluating the performance of our object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1762295140527,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "9TNGKuSO-X-G",
    "outputId": "7814bdcf-4e46-4955-ea7f-baa09272b296"
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "#Normalize our manual GT to a standard format\n",
    "#=============================================\n",
    "import pandas as pd\n",
    "\n",
    "GT_CSV_RAW = \"/content/ground_truth.csv.csv\"\n",
    "GT_CSV_STD = \"/content/eval_ground_truth_std.csv\"\n",
    "\n",
    "gt_raw = pd.read_csv(GT_CSV_RAW)\n",
    "\n",
    "# Convert (x, y, w, h) -> (x1, y1, x2, y2)\n",
    "gt_std = pd.DataFrame({\n",
    "    \"image_name\": gt_raw[\"image_name\"],\n",
    "    \"label\": gt_raw[\"label_name\"].astype(str).str.strip(),\n",
    "    \"x1\": gt_raw[\"bbox_x\"],\n",
    "    \"y1\": gt_raw[\"bbox_y\"],\n",
    "    \"x2\": gt_raw[\"bbox_x\"] + gt_raw[\"bbox_width\"],\n",
    "    \"y2\": gt_raw[\"bbox_y\"] + gt_raw[\"bbox_height\"]\n",
    "})\n",
    "\n",
    "#  corrections for label typos / harmonization (edit as needed)\n",
    "label_fix = {\n",
    "    \"microvalve\": \"microwave\",\n",
    "    \"tvmonitor\": \"tv\"\n",
    "}\n",
    "gt_std[\"label\"] = gt_std[\"label\"].str.lower().replace(label_fix)\n",
    "\n",
    "gt_std.to_csv(GT_CSV_STD, index=False)\n",
    "print(f\" Saved normalized GT to {GT_CSV_STD} (rows: {len(gt_std)})\")\n",
    "gt_std.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVZS9_tl-puo"
   },
   "source": [
    "Next I compute per-class Precision, Recall, and F1 score for our YOLOv8 object detection results using the normalized manual ground truth data, based on an IoU threshold of 0.5. This is a core part of evaluating how well our object detection model performed.\n",
    "\n",
    "Here's what this code does:\n",
    "\n",
    "Define Paths and Threshold: I set the paths for our YOLO detections CSV (/content/eval_detections_yolo.csv) and the standardized ground truth CSV (/content/eval_ground_truth_std.csv). I also define the IOU_THRESH as 0.5. Load Data: I load both the detections and the ground truth data into pandas DataFrames. Normalize Labels: I ensure that the labels in both DataFrames are in lowercase to facilitate accurate matching. IoU Function: A helper function iou_xyxy is defined to calculate the Intersection over Union between two bounding boxes in the (x1, y1, x2, y2) format. Evaluation Loop: I get a list of all unique object classes present in our ground truth. I initialize counters for True Positives (TP), False Positives (FP), and False Negatives (FN) for both per-class and overall calculations. I loop through each unique class: I filter both the ground truth and detection DataFrames to only include the current class. I then group the data by image_name to compare detections and ground truth within the same frame. For each image, I perform a greedy matching process: I iterate through each ground truth box for that image. For each ground truth box, I find the detection box from the same image with the highest IoU that hasn't already been used. If the best IoU is above the IOU_THRESH, it's counted as a TP, and the detection is marked as used. If no detection matches the ground truth box above the threshold, it's a FN. Any detection boxes that were not matched to a ground truth box are counted as FPs. After processing all images for a class, I calculate the Precision, Recall, and F1 score for that class using the accumulated TP, FP, and FN counts. I also accumulate the overall TP, FP, and FN counts across all classes. Summarize and Save: I create a pandas DataFrame (sum_df) containing the evaluation metrics (TP, FP, FN, precision, recall, F1) for each class. This DataFrame is sorted by F1 score in descending order and saved to /content/task2_eval_per_class.csv. I calculate the overall micro-averaged precision, recall, and F1 score using the total TP, FP, and FN counts. These overall metrics are saved to /content/task2_eval_overall.csv. Display Results: I print a header indicating the evaluation results and display the top 15 rows of the per-class summary DataFrame and the overall metrics DataFrame. Finally, I print the paths where the results were saved. Interpretation of the Output:\n",
    "\n",
    "The output we see shows the evaluation results for our YOLOv8 object detection model against the manual ground truth:\n",
    "\n",
    "=== Task 2 evaluation (IoU>=0.5) ===: This header indicates the start of the evaluation summary.\n",
    "\n",
    "The first displayed table (sum_df) shows the performance metrics (TP, FP, FN, precision, recall, F1) for each detected class, sorted by F1 score. We can see which classes the model performed best on (e.g., 'orange', 'cup', 'refrigerator') and which it struggled with (e.g., 'fork', 'bowl', 'dining table').\n",
    "\n",
    "The second displayed table (overall) shows the micro-averaged metrics across all classes: TP: 72: Total True Positives (correctly detected objects matched to ground truth).\n",
    "\n",
    "FP: 23: Total False Positives (detections that did not match any ground truth object).\n",
    "\n",
    "FN: 135: Total False Negatives (ground truth objects that were not detected). precision_micro: 0.758: Overall precision (out of all detections, ~76% were correct).\n",
    "\n",
    "recall_micro: 0.348: Overall recall (out of all actual objects in the ground truth, ~35% were detected).\n",
    "\n",
    "f1_micro: 0.477: Overall F1 score, a harmonic mean of precision and recall. The output confirms that the evaluation process ran successfully and provides detailed metrics on the performance of our object detection model, both per class and overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1762295232928,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "UL544JoH-rxf",
    "outputId": "b6d27cfc-68f3-4b25-bc02-3247affd8b4a"
   },
   "outputs": [],
   "source": [
    "#=======================================================\n",
    "#Compute per-class Precision / Recall / F1 using IoUâ‰¥0.5\n",
    "#=======================================================\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "DET_CSV = \"/content/eval_detections_yolo.csv\"\n",
    "GT_CSV  = \"/content/eval_ground_truth_std.csv\"\n",
    "IOU_THRESH = 0.5\n",
    "\n",
    "det = pd.read_csv(DET_CSV)\n",
    "gt  = pd.read_csv(GT_CSV)\n",
    "\n",
    "# keep labels lowercase for exact matching\n",
    "det[\"label\"] = det[\"label\"].str.lower()\n",
    "gt[\"label\"]  = gt[\"label\"].str.lower()\n",
    "\n",
    "def iou_xyxy(a, b):\n",
    "    xA, yA = max(a[0], b[0]), max(a[1], b[1])\n",
    "    xB, yB = min(a[2], b[2]), min(a[3], b[3])\n",
    "    inter  = max(0, xB-xA)*max(0, yB-yA)\n",
    "    if inter <= 0: return 0.0\n",
    "    areaA = (a[2]-a[0])*(a[3]-a[1])\n",
    "    areaB = (b[2]-b[0])*(b[3]-b[1])\n",
    "    return inter / (areaA + areaB - inter + 1e-9)\n",
    "\n",
    "classes = sorted(gt[\"label\"].unique())\n",
    "summary = []\n",
    "TP_tot = FP_tot = FN_tot = 0\n",
    "\n",
    "for cls in classes:\n",
    "    gtc = gt[gt[\"label\"]==cls]\n",
    "    detc = det[det[\"label\"]==cls]\n",
    "\n",
    "    TP = FP = FN = 0\n",
    "    # group by image to match within the same frame\n",
    "    imgs = sorted(set(gtc[\"image_name\"]) | set(detc[\"image_name\"]))\n",
    "    for img in imgs:\n",
    "        gtb = gtc[gtc[\"image_name\"]==img][[\"x1\",\"y1\",\"x2\",\"y2\"]].to_numpy(float)\n",
    "        prb = detc[detc[\"image_name\"]==img][[\"x1\",\"y1\",\"x2\",\"y2\"]].to_numpy(float)\n",
    "\n",
    "        used_pred = set()\n",
    "        # greedy matching\n",
    "        for gi, g in enumerate(gtb):\n",
    "            best_iou, best_pi = 0.0, -1\n",
    "            for pi, p in enumerate(prb):\n",
    "                if pi in used_pred: continue\n",
    "                v = iou_xyxy(g, p)\n",
    "                if v > best_iou:\n",
    "                    best_iou, best_pi = v, pi\n",
    "            if best_iou >= IOU_THRESH:\n",
    "                TP += 1\n",
    "                used_pred.add(best_pi)\n",
    "            else:\n",
    "                FN += 1\n",
    "        FP += max(0, len(prb) - len(used_pred))\n",
    "\n",
    "    prec = TP / (TP + FP + 1e-9)\n",
    "    rec  = TP / (TP + FN + 1e-9)\n",
    "    f1   = 2*prec*rec / (prec+rec + 1e-9)\n",
    "    summary.append({\"class\": cls, \"TP\":TP, \"FP\":FP, \"FN\":FN,\n",
    "                    \"precision\":prec, \"recall\":rec, \"f1\":f1})\n",
    "\n",
    "    TP_tot += TP; FP_tot += FP; FN_tot += FN\n",
    "\n",
    "sum_df = pd.DataFrame(summary).sort_values(\"f1\", ascending=False)\n",
    "sum_df.to_csv(\"/content/task2_eval_per_class.csv\", index=False)\n",
    "\n",
    "prec_micro = TP_tot/(TP_tot+FP_tot+1e-9)\n",
    "rec_micro  = TP_tot/(TP_tot+FN_tot+1e-9)\n",
    "f1_micro   = 2*prec_micro*rec_micro / (prec_micro+rec_micro + 1e-9)\n",
    "\n",
    "overall = pd.DataFrame([{\n",
    "    \"TP\": TP_tot, \"FP\": FP_tot, \"FN\": FN_tot,\n",
    "    \"precision_micro\": prec_micro, \"recall_micro\": rec_micro, \"f1_micro\": f1_micro\n",
    "}])\n",
    "overall.to_csv(\"/content/task2_eval_overall.csv\", index=False)\n",
    "\n",
    "print(\"=== Task 2 evaluation (IoU>=0.5) ===\")\n",
    "display(sum_df.head(15))\n",
    "print(\"\\nOverall (micro):\")\n",
    "display(overall)\n",
    "print(\"Saved: /content/task2_eval_per_class.csv, /content/task2_eval_overall.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1762295470656,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "_VxCJUfj_LWw",
    "outputId": "e09b119f-a992-416c-eab3-e4084f41793f"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION: CONFUSION MATRIX (Simulated)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nGenerating confusion matrix visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Detection Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Top 10 classes confusion-like analysis\n",
    "ax = axes[0]\n",
    "# Corrected variable name from predictions_df to det\n",
    "top_10_classes = det['label'].value_counts().head(10)\n",
    "confidence_by_class = det.groupby('label')['conf'].mean().loc[top_10_classes.index]\n",
    "\n",
    "bars = ax.bar(range(len(top_10_classes)), confidence_by_class.values,\n",
    "             color=plt.cm.RdYlGn(confidence_by_class.values / confidence_by_class.max()),\n",
    "             edgecolor='black', linewidth=1.5)\n",
    "ax.set_xticks(range(len(top_10_classes)))\n",
    "ax.set_xticklabels(confidence_by_class.index, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Average Confidence', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 10 Classes: Average Detection Confidence', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Detection count vs Confidence scatter\n",
    "ax = axes[1]\n",
    "class_stats_list = []\n",
    "# Corrected variable name from predictions_df to det\n",
    "for class_name in det['label'].unique():\n",
    "    # Corrected variable name from predictions_df to det\n",
    "    class_data = det[det['label'] == class_name]\n",
    "    class_stats_list.append({\n",
    "        'class': class_name,\n",
    "        'count': len(class_data),\n",
    "        'avg_conf': class_data['conf'].mean(),\n",
    "        'std_conf': class_data['conf'].std()\n",
    "    })\n",
    "\n",
    "class_stats_df = pd.DataFrame(class_stats_list)\n",
    "# Ensure 'count' column exists before sorting\n",
    "if 'count' in class_stats_df.columns:\n",
    "    class_stats_df = class_stats_df.sort_values('count').reset_index(drop=True)\n",
    "\n",
    "scatter = ax.scatter(class_stats_df['count'], class_stats_df['avg_conf'],\n",
    "                    s=200, alpha=0.6, c=class_stats_df['count'],\n",
    "                    cmap='viridis', edgecolors='black', linewidth=1.5)\n",
    "ax.set_xlabel('Number of Detections', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Average Confidence Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Detection Count vs Confidence Correlation', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Detection Count', fontsize=11)\n",
    "\n",
    "# Add trend line\n",
    "# Ensure there are enough data points to calculate a trend line\n",
    "if len(class_stats_df) > 1:\n",
    "    z = np.polyfit(class_stats_df['count'], class_stats_df['avg_conf'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(class_stats_df['count'], p(class_stats_df['count']),\n",
    "           \"r--\", linewidth=2, label=f'Trend: y={z[0]:.4f}x+{z[1]:.4f}')\n",
    "    ax.legend(fontsize=10)\n",
    "else:\n",
    "    print(\"Not enough data points to plot a trend line.\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/evaluation/performance_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"âœ“ Saved: performance_analysis.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 1229,
     "status": "ok",
     "timestamp": 1762295533824,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "nynSK_lP_vnR",
    "outputId": "fb807bc6-c9a3-4fa6-e235-e3e63593210b"
   },
   "outputs": [],
   "source": [
    "# === generating/content/evaluation/performance_analysis.png ===\n",
    "import os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "os.makedirs(\"/content/evaluation\", exist_ok=True)\n",
    "\n",
    "DET_CSV = \"/content/eval_detections_yolo.csv\"  # created earlier\n",
    "assert os.path.exists(DET_CSV), \"Missing eval_detections_yolo.csv. Run the detection export step first.\"\n",
    "\n",
    "det = pd.read_csv(DET_CSV)\n",
    "det['label'] = det['label'].str.lower()\n",
    "\n",
    "# Per-class stats\n",
    "counts = det.groupby('label').size().rename(\"count\")\n",
    "avg_conf = det.groupby('label')['conf'].mean().rename(\"avg_conf\")\n",
    "stats = pd.concat([counts, avg_conf], axis=1).sort_values(\"count\", ascending=False)\n",
    "\n",
    "# Top-10 by count for the bar chart\n",
    "top10 = stats.head(10).sort_values(\"avg_conf\", ascending=False)\n",
    "\n",
    "# ---- Plot ----\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.suptitle(\"Detection Performance Analysis\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Left: Top-10 average confidence\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "sns.barplot(x=\"avg_conf\", y=top10.index, data=top10.reset_index(), ax=ax1)\n",
    "ax1.set_xlabel(\"Average Confidence\", fontweight=\"bold\"); ax1.set_ylabel(\"Class\", fontweight=\"bold\")\n",
    "ax1.set_title(\"Top 10 Classes: Average Detection Confidence\")\n",
    "\n",
    "# Right: count vs confidence correlation (all classes)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "sc = ax2.scatter(stats[\"count\"], stats[\"avg_conf\"], c=stats[\"count\"], cmap=\"viridis\", s=70)\n",
    "# Trend line\n",
    "z = np.polyfit(stats[\"count\"], stats[\"avg_conf\"], 1)\n",
    "x_line = np.linspace(stats[\"count\"].min(), stats[\"count\"].max(), 50)\n",
    "ax2.plot(x_line, z[0]*x_line + z[1], \"r--\", linewidth=1, label=f\"Trend: y={z[0]:.4f}x+{z[1]:.4f}\")\n",
    "ax2.set_xlabel(\"Number of Detections\", fontweight=\"bold\"); ax2.set_ylabel(\"Average Confidence\", fontweight=\"bold\")\n",
    "ax2.set_title(\"Detection Count vs Confidence Correlation\"); ax2.legend()\n",
    "cbar = plt.colorbar(sc, ax=ax2); cbar.set_label(\"Detection Count\")\n",
    "plt.tight_layout(rect=[0,0,1,0.95])\n",
    "\n",
    "out_path = \"/content/evaluation/performance_analysis.png\"\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.show()\n",
    "print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1762296423811,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "NTOGlOacAIC6",
    "outputId": "b925eef5-8183-433f-aff1-5d8e7b5142be"
   },
   "outputs": [],
   "source": [
    "# === Generating Confusion Matrix ===\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load evaluation results per class\n",
    "eval_per_class_path = \"/content/task2_eval_per_class.csv\"\n",
    "if not os.path.exists(eval_per_class_path):\n",
    "    print(f\"Error: Evaluation file not found at {eval_per_class_path}. Please run the evaluation step first.\")\n",
    "else:\n",
    "    eval_df = pd.read_csv(eval_per_class_path)\n",
    "\n",
    "    # Create a confusion matrix-like structure\n",
    "    # For object detection evaluation per class, the TP, FP, FN counts are more informative\n",
    "    # than a traditional classification confusion matrix. We can visualize these counts.\n",
    "\n",
    "    # Prepare data for visualization\n",
    "    # Adding a column for 'Total Ground Truth' (TP + FN)\n",
    "    eval_df['Total_GT'] = eval_df['TP'] + eval_df['FN']\n",
    "    # Adding a column for 'Total Detections' (TP + FP)\n",
    "    eval_df['Total_Detections'] = eval_df['TP'] + eval_df['FP']\n",
    "\n",
    "    # Select relevant columns and melt for plotting\n",
    "    plot_df = eval_df[['class', 'TP', 'FP', 'FN']].melt(\n",
    "        id_vars='class',\n",
    "        var_name='Metric',\n",
    "        value_name='Count'\n",
    "    )\n",
    "\n",
    "    # Sort classes by F1 score for consistent visualization (optional)\n",
    "    # We'll use the order from the original eval_df which is sorted by F1\n",
    "    class_order = eval_df['class'].tolist()\n",
    "    plot_df['class'] = pd.Categorical(plot_df['class'], categories=class_order, ordered=True)\n",
    "    plot_df = plot_df.sort_values('class')\n",
    "\n",
    "    # Plotting the per-class TP, FP, FN as a grouped bar chart (similar to a confusion matrix breakdown)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Count', y='class', hue='Metric', data=plot_df, palette='viridis')\n",
    "    plt.title('Per-Class Detection Metrics (TP, FP, FN)', fontsize=16)\n",
    "    plt.xlabel('Count', fontsize=12)\n",
    "    plt.ylabel('Object Class', fontsize=12)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    confusion_matrix_path = \"/content/evaluation/per_class_metrics_bar_chart.png\"\n",
    "    plt.savefig(confusion_matrix_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved per-class metrics bar chart to: {confusion_matrix_path}\")\n",
    "\n",
    "    # --- Explanation on ROC Curves and Precision-Recall Curves ---\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ROC Curves vs Precision-Recall Curves in Object Detection\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ROC (Receiver Operating Characteristic) curves plot True Positive Rate (Recall) against False Positive Rate.\")\n",
    "    print(\"They are commonly used in binary classification tasks.\")\n",
    "    print(\"\\nIn multi-class object detection, where there can be multiple objects of different classes in an image,\")\n",
    "    print(\"a single global False Positive Rate is less intuitive.\")\n",
    "    print(\"Instead, metrics like Precision-Recall curves and Average Precision (AP) are typically used.\")\n",
    "    print(\"\\nPrecision-Recall curves plot Precision against Recall for different confidence thresholds.\")\n",
    "    print(\"A high AP score indicates good performance across various recall levels.\")\n",
    "    print(\"\\nWhile a direct ROC curve like in binary classification is not standard for object detection evaluation\")\n",
    "    print(\"(especially not per class in the same way), the Precision and Recall values we computed earlier\")\n",
    "    print(\"are fundamental components used in object detection evaluation metrics like mAP (mean Average Precision).\")\n",
    "    print(\"\\nTo create Precision-Recall curves, you would typically need to collect detection results across\")\n",
    "    print(\"a range of confidence thresholds and calculate precision and recall at each threshold.\")\n",
    "    print(\"This requires a more extensive evaluation process than the single-threshold evaluation done here.\")\n",
    "    print(\"\\nHowever, you can visualize the Precision and Recall values we calculated for each class:\")\n",
    "\n",
    "    print(f\"Saved per-class Precision and Recall bar chart to: {pr_bar_chart_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1762300143067,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "CWd8PmSkRfbX"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load trained model (replace with your own path if needed)\n",
    "model = YOLO('yolov8n.pt')  # or yolov8n_custom.pt if you fine-tuned\n",
    "\n",
    "# Save/export model weights\n",
    "model.save('yolov8_trained.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1762300189151,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "TZZqs967Rl8n",
    "outputId": "df85aa49-f1f4-488b-a0fb-dbdca1d78e68"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1762300257364,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "wNCkoJuqR8os",
    "outputId": "d8715d0e-8a6d-455a-95f5-84d5860472ab"
   },
   "outputs": [],
   "source": [
    "!zip -r trained_model.zip yolov8_trained.pt  # or model.pkl\n",
    "from google.colab import files\n",
    "files.download('trained_model.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1762302489951,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "Ij_Kw7y8aXNY",
    "outputId": "94148baf-7439-4d01-b4de-57418f1fa9c0"
   },
   "outputs": [],
   "source": [
    "!du -h /content/object detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1762302659136,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "3BQLuujabHXM",
    "outputId": "0a3b015d-c37a-418d-f2fd-ed210c6690d2"
   },
   "outputs": [],
   "source": [
    "import json, os, io, IPython\n",
    "nb = IPython.get_ipython().get_parent()['content']  # current notebook JSON\n",
    "raw = json.dumps(nb).encode(\"utf-8\")\n",
    "print(\"Notebook size (MB):\", len(raw)/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2917,
     "status": "ok",
     "timestamp": 1762302899694,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "ps6X5w8JcBNG",
    "outputId": "2ee0bc48-9e13-49bd-ae7e-ef685fc1b29e"
   },
   "outputs": [],
   "source": [
    "!zip -r model_files.zip yolov8_trained.pt model.pkl trained_model.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4108,
     "status": "ok",
     "timestamp": 1762304196046,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "5I7RkaYzghg7",
    "outputId": "77fe1d36-9953-4a9c-f2c5-92bd659b4027"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace \"object_detection.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1762304654684,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "7wZOIYB6ihzR"
   },
   "outputs": [],
   "source": [
    "!ls -lh /content | grep ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3823,
     "status": "ok",
     "timestamp": 1762304823258,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "05e6cd0d",
    "outputId": "4c0c192b-4bb9-49dd-99fd-88ac75e9541b"
   },
   "outputs": [],
   "source": [
    "# Save the current notebook to /content/object-detection.ipynb\n",
    "!jupyter nbconvert --to notebook --output-dir /content --output object-detection.ipynb \"/content/YOUR_NOTEBOOK_NAME.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3909,
     "status": "ok",
     "timestamp": 1762305550588,
     "user": {
      "displayName": "Zara",
      "userId": "16167798281749510134"
     },
     "user_tz": -660
    },
    "id": "BlILVLcIk6GN",
    "outputId": "067bc829-3fe9-40e2-e4d5-815b705f780c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- IMPORTANT: FIX THIS PATH ---\n",
    "# Replace \"/path/to/your/current/notebook.ipynb\" with the ACTUAL path to THIS notebook file.\n",
    "# Find the correct path in the Colab file explorer (folder icon on the left).\n",
    "# Example: If your notebook is in Google Drive, the path might look like\n",
    "# \"/content/drive/MyDrive/Your Notebook Name.ipynb\"\n",
    "notebook_path = \"/path/to/your/current/notebook.ipynb\" # <--- *** UPDATE THIS LINE ***\n",
    "\n",
    "output_name = os.path.basename(notebook_path).replace(\".ipynb\", \"_clean.ipynb\")\n",
    "output_dir = \"/content\"\n",
    "output_path = os.path.join(output_dir, output_name)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to save cleaned notebook from: {notebook_path} to {output_path}\")\n",
    "\n",
    "# Use nbconvert to save a copy with outputs cleared\n",
    "# The --ClearOutputPreprocessor.enabled=True flag removes cell outputs\n",
    "# The --to notebook specifies the output format is a notebook\n",
    "# The --output-dir specifies the directory to save the output\n",
    "# The --output specifies the name of the output file\n",
    "# The last argument is the path to the source notebook\n",
    "os.system(f'jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to notebook --output-dir \"{output_dir}\" --output \"{output_name}\" \"{notebook_path}\"')\n",
    "\n",
    "# Check if the file was created\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"âœ… Cleaned notebook saved successfully at: {output_path}\")\n",
    "    # Optional: Download the cleaned notebook\n",
    "    # from google.colab import files\n",
    "    # files.download(output_path)\n",
    "else:\n",
    "    print(f\"âŒ Failed to save cleaned notebook to: {output_path}.\")\n",
    "    print(f\"Please verify that the 'notebook_path' variable is set to the correct path of the current notebook.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPfoKyfcFn8PFxM9lZtQ1V9",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1yEjB8TX0yUlQrzsm_QAMIXH7UVw085Xq",
     "timestamp": 1762305798613
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
